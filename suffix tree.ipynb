{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration of what the library does:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{8, 0}\n",
      "abc\n",
      "{32, 3, 40, 14, 22}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from suffix_trees import STree\n",
    "\n",
    "# Suffix-Tree example.\n",
    "st = STree.STree(\"abcdefghab\")\n",
    "print(st.find(\"abc\")) # 0\n",
    "print(st.find_all(\"ab\")) # [0, 8]\n",
    "\n",
    "# Generalized Suffix-Tree example.\n",
    "a = [\"xxxabcxxx\", \"adsaabc\", \"ytysabcrew\", \"qqqabcqw\", \"aaabc\"]\n",
    "st = STree.STree(a)\n",
    "print(st.lcs()) # \"abc\"\n",
    "print(st.find_all(\"abc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actual Program Starts from here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % similarity = 10.526315789473683\n",
      "Thia means 10.526315789473683 % of doc1 is similar with doc2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "import re\n",
    "from suffix_trees import STree\n",
    "\n",
    "# Opening of file and converting into a single string \n",
    "\n",
    "def SingleString(lines):    \n",
    "    temp = \"\"\n",
    "    for i in range(0, len(lines)):\n",
    "        if (lines[i] != \"\\n\"):\n",
    "            temp = temp + lines[i]\n",
    "    temp = temp.replace(\"\\n\", \" \")\n",
    "    return temp\n",
    "\n",
    "# Preproceesing text\n",
    "\n",
    "def fullCleanup(doc):  #This function is useful for cosine similarity, and substring matching\n",
    "    d=doc.replace('\\n',' ');\n",
    "    L=re.split(r'\\W+',d);\n",
    "    L=[i.lower() for i in L];\n",
    "    bucket=[];\n",
    "    stop_words=set(stopwords.words('english'));\n",
    "    for i in L:\n",
    "        if i in stop_words or len(i)<=2:\n",
    "            continue;\n",
    "        else:\n",
    "            bucket+=[i];\n",
    "    lemmatiser=WordNetLemmatizer();\n",
    "    bucket=[lemmatiser.lemmatize(i,pos='v') for i in bucket];\n",
    "    bucket=[lemmatiser.lemmatize(i,pos='n') for i in bucket];\n",
    "    bucket=[lemmatiser.lemmatize(i,pos=wordnet.ADJ) for i in bucket];\n",
    "    bucket=[lemmatiser.lemmatize(i,pos=wordnet.ADV) for i in bucket];\n",
    "    return bucket;\n",
    "\n",
    "\n",
    "# match exact string instead of sub-string \n",
    "\n",
    "def match(index , w ):\n",
    "    match = w\n",
    "    count = 0\n",
    "    for i in range(0 , len(index)):\n",
    "        k=index[i]-1\n",
    "        j=len(w)+k+1\n",
    "        if j<(len(pre_data1)-1):\n",
    "            while pre_data1[j] != \" \" and pre_data1[k] == \" \":\n",
    "                match = match + pre_data1[j]\n",
    "                if j < (len(pre_data1)-1):\n",
    "                    j = j + 1\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "    if (match == w):\n",
    "            count = count+1\n",
    "    return count\n",
    "\n",
    "# Driver code\n",
    "\n",
    "f1 = open(\"smallTxt\", \"r\")\n",
    "fl1 = f1.readlines()\n",
    "f2 = open(\"smallTxt2\", \"r\")\n",
    "fl2 = f2.readlines()\n",
    "\n",
    "# pre-proceesed data\n",
    "\n",
    "list1 = fullCleanup(SingleString(fl1))\n",
    "list2 = fullCleanup(SingleString(fl2))\n",
    "\n",
    "#list1 = fullCleanup(\"I love fruits like apple, banana, grapes, mango, chiku, strawberry  and so on\")\n",
    "#list2 = fullCleanup(\"I love vegetables potato, lady finger, cauliflower, cabbage  and  fruits like apple, banana, grapes, mango and many more\")\n",
    "pre_data1 = \" \".join(list1)\n",
    "\n",
    "pre_data2 = \" \".join(list2)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "# Create tree for file1 , i.e , pre_data1\n",
    "\n",
    "st = STree.STree(pre_data1)\n",
    "\n",
    "# search each word of pre_data2 in the tree of pre_data1\n",
    "\n",
    "\n",
    "matchcount = 0\n",
    "total = []\n",
    "\n",
    "for word in list2:\n",
    "    if word not in total:\n",
    "        total.append(word)\n",
    "        if st.find_all(word):\n",
    "            matchcount = matchcount + match(list(st.find_all(word)), word)\n",
    "\n",
    "    \n",
    "similarity =  (matchcount / len(list1) )* 100\n",
    "\n",
    "print(\" % similarity =\" , similarity)\n",
    "print(\"Thia means\",similarity,\"% of doc1 is similar with doc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
